---
title: "STAT 6620 HW#5"
author: "Lanqin Zhao,   NetID: ae8275"
output: html_notebook
---

### Estimating the quality of wines with regression trees and model trees

## Step 1 - collecting data ----
we will use redwines.csv dataset donated to the UCI Machine Learning Data Repository (http://archive.ics.uci.edu/ml) by P. Cortez, A.Cerdeira, F. Almeida, T. Matos, and J. Reis.

## Step 2: Exploring and preparing the data ----
Explore the data to see whether we can use the data to predict and prepare the data for modeling.
# Explore data
Import data

```{r}
#Import the CSV file.
wine <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml10/redwines.csv")

# examine the wine data
str(wine)

```
#check for distribution of dependent variable, quality. 
The wine quality values appear to follow a fairly normal, bell-shaped distribution,centered around a value of six. This makes sense intuitively because most wines are
of average quality; few are particularly bad or good

```{r}
# the distribution of quality ratings
hist(wine$quality)

```
#Check variables
```{r}
# summary statistics of the wine data
summary(wine)

```
#Creating training and testing datasets
```{r}

# split into training and testing datasets
wine_train <- wine[1:1250, ]
wine_test <- wine[1251:1599, ]

```

## Step 3: Training a model on the data ----
Using regression tree to train a model on the data. 
# fit a model
```{r}
# regression tree using rpart
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)

# get basic information about the tree
m.rpart

# get more detailed information about the tree
summary(m.rpart)
```
# Visualizing decision trees

```{r}
# use the rpart.plot package to create a visualization
library(rpart.plot)

# a basic decision tree diagram
rpart.plot(m.rpart, digits = 3)

# a few adjustments to the diagram
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
```


## Step 4: Evaluating model performance ----
We use three statistical methode to evaluate the performance: 
1. compare distributions of predicted values vs. actual values
2. Correlation
3. mean absolute error
Given the preceding three performance indicators, our model is performing fairly well.
```{r}
# generate predictions for the testing dataset
p.rpart <- predict(m.rpart, wine_test)

```
# compare distributions of predicted values vs. actual values
The summary statistics of our predictions suggests a potential problem; the predictions fall on a much narrower range than the true values. This suggests that the model is not correctly identifying the extreme cases, in
particular the best and worst wines. On the other hand, between the first and third
quartile, we may be doing well.

```{r}
# compare the distribution of predicted values vs. actual values
summary(p.rpart)
summary(wine_test$quality)
```
# Correlation of predicted values and actual values
The correlation is 0.56, which is acceptable.
```{r}
# compare the correlation
cor(p.rpart, wine_test$quality)
```
#  the mean absolute error
The MAE for our predictions is o.55. This implies that, on average, the difference between our model's predictions and the true quality score was about 0.55. On a quality scale from zero to 10, this seems to
suggest that our model is doing fairly well.
```{r}
# function to calculate the mean absolute error
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}

# mean absolute error between predicted and actual values
MAE(p.rpart, wine_test$quality)

# mean absolute error between actual values and mean value
mean(wine_train$quality) # result = 5.64
MAE(5.87, wine_test$quality)
```

## Step 5: Improving model performance ----
To improve the performance of our learner, let's try to build a model tree. Recall that a model tree improves on regression trees by replacing the leaf nodes with regression models. This often results in more accurate results than regression trees, which use only a single value for prediction at the leaf nodes.
The summary statistics of our predictions suggests that the predictions fall on a much wider range than that the predicted values the regression tree produced.The correlation is 0.66,much higher than 0.56 before. And the MAE is 0.49, which is lower than 0.55 the previous regression tree model produced, so the performance is improved a lot.
```{r}

# train a M5' Model Tree
library(RWeka)
m.m5p <- M5P(quality ~ ., data = wine_train)

# display the tree
m.m5p

# get a summary of the model's performance
summary(m.m5p)
summary(wine_test$quality)

# generate predictions for the model
p.m5p <- predict(m.m5p, wine_test)

# summary statistics about the predictions
summary(p.m5p)

# correlation between the predicted and true values
cor(p.m5p, wine_test$quality)

# mean absolute error of predicted and true values
# (uses a custom function defined above)
MAE(wine_test$quality, p.m5p)
```



